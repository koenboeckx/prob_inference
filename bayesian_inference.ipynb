{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.6 64-bit ('venv': venv)",
   "display_name": "Python 3.6.6 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "b4aece322c8117298b0ce22122307cb9232acf2728693fdb1f1a02fcb3254986"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.ritchievink.com/blog/2019/06/10/bayesian-inference-how-we-are-able-to-chase-the-posterior/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference: How we are able to chase the Posterior\n",
    "## Bayes' Formula\n",
    "Bayes' Formula is a way to reverse a conditional probability:\n",
    "$$\n",
    "P(\\theta | D) = \\frac{P(D | \\theta) P(\\theta)}{P(D)}\n",
    "$$\n",
    "If we have a model architecture and we want to find the most likely parameters $\\theta$ (which is a large vector, containing all parameters of the model) for that model, given a set of observed data points $D$. This is what we are interested in, and is called the *posterior* $P(\\theta|D)$. We often have some prior belief about the parameters of our model: the *prior* $P(\\theta)$. Given all values of $\\theta$, we can compute the probability of observering the data points. This is the *likelihood* $P(D|\\theta)$. The last term is the *evidence* $P(D)$. This term is hard to compute since it is the marginal likelihood when all parameters are marginalized:\n",
    "$$\n",
    "P(D) = \\int_{\\theta} P(D|\\theta) P(\\theta) d\\theta\n",
    "$$\n",
    "Even for moderately high dimensions of $\\theta$ the amount of numerical operations explodes.\n",
    "\n",
    "## A Simple Example\n",
    "Let’s base this post on a comprehensible example. We will do a full Bayesian analysis in Python by computing the posterior. Later we will assume that we cannot do this. Therefore we will approximate the posterior (we’ve computed) with MCMC and Variational Inference.\n",
    "\n",
    "Assume we have observed two data points: $D = \\{195, 182\\}$. Both are the observed lenghts (in cm) of men in a basketball competion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "lengths = np.array([195, 182])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Likelihood function\n",
    "\n",
    "We'll assume that the distribution of the true weights (the posterior) follows a Gaussian distribution. A Gaussian is parametrized with mean $\\mu$ and variance $\\sigma^2$. For a reasonable domain of these parameters $\\theta = \\{\\mu, \\sigma \\}$, we can compute the likelihood $P(D|\\theta) = P(D|\\mu, \\sigma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation domain\n",
    "\n",
    "# lets create a grid of our two parameters\n",
    "mu = np.linspace(150, 250)\n",
    "sigma = np.linspace(0, 15)[::-1]\n",
    "\n",
    "mm, ss = np.meshgrid(mu, sigma)  # just broadcasted parameters\n",
    "\n",
    "# the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}